\section{Current application-OS abstractions}

Modern operating systems give the illusion to applications that they have dedicated access to all resources. In a typical operating system, an application has a dedicated user address space with a set of system calls to access system resources and services. The operating system provides the hardware abstraction layer to virtualize all hardware so the application may assume it has complete control over the entire machine.

On the other side of the barrier, the operating system kernel is responsible for running applications and providing them with address spaces along with CPU and other hardware resources. The kernel treats applications as black boxes to isolate and manage.

\subsection{These abstractions are inadequate}
However, such abstractions encourage applications to assume full access when they do not truly know what hardware resources are available to them at a given time. In the case of parallel programming, the concept meant to clarify may actually obfuscate. As a simple example, developers will frequently structure their applications to provision as many threads as the hardware will allow.

Further inspection reveals that the abstraction barrier can be breached in many ways. Applications can use a variety of directives and system calls to learn about underlying resources and the operating system can monitor application behavior or receive hints via those same system calls. These mechanisms suggest via proof by existence that the abstraction barrier simply is not adequate in many use cases.

\subsection{Increasing core counts and co-tenancy will exacerbate the inadequacies}

As core counts increase, individual applications struggle to make efficient use of all available resources. The newest Intel boards sport nearly 300 general-purpose cores. As cluster managers and cloud providers migrate to these manycore chips, they become burdened with the economic need to saturate their hardware. Packing many applications onto a single node becomes a increasingly common reality. These trends strain the illusion that applications still have their exclusive access to hardware.






\begin{itemize}
  \item For example, the Linux scheduler treats applications almost as black boxes. Schedules threads independently (limited within a process e.g., where threads / processes are spawned may give some cache locality). Used to have some heuristics to determine interactivity, but CFS no longer includes~\cite{molnar2007cfs}. The traditional approach to improve the utilization for multi-core or many-core processors is to schedule ready threads to run available CPU cores [cite ...].  ...
  \item Some control with cgroups to coordinate groups of threads, but not perfect (e.g., if there are too many threads, the scheduler cannot schedule them all within a single epoch without going below the minimum time granularity, then the epoch will increase; still `fair', but cycle is longer).
  \item Further abstraction from hardware (e.g., containers/VMs): application gets illusion of some resources that may be far from reality; OS treats applications as black boxes
\end{itemize}

\subsection{Even today, the abstraction is inadequate}
Because we break it all the time. As evidenced by:
\begin{itemize}
\item Hardware can get information / specific instructions from applications re how to do its job via directives and other DSLs (e.g., \texttt{LIKELY}, \texttt{UNLIKELY}, \texttt{atomic})
\item OS can get hints from application via sys calls (e.g., setting processor affinity)
\item Applications can get information about resources via syscalls, hardware/software counters
\end{itemize}

\begin{itemize}
	\item Under-provisioning threads can lead to low resource utilization
    \item Over-provisioning can create contention for resources and the system may thrash
 	\item Over-provisioning may cause low utilization on many core (not in the sense of having idle CPUs, but in the sense of eating up more total CPU to complete the job than at lower level of parallelism; need to explain this clearly so people aren't confused by `low utiliation'). CPU share and quota (cgroup) not enough; need to change parallelism of applications to affect utilization (different overheads / slow-down in total CPU time). different applications have various speedup curves [cite splash, PARSEC], depending on the granularities of computational components, cache localities and synchronization overheads and so on.  Applications with relatively poor speedup curves do not scale well on a large number of CPU cores.  In other words, it does not utilize the parallel processing power.
\end{itemize}

\subsection{Attempting to infer information across the barrier implicitly is hard}
\begin{itemize}
\item OS inferring about application. History of Linux scheduler. O(1) scheduler tried to classify applications based on behavior. Got so complicated that scrapped in favor of CFS.
\item Application inferring about hardware. Can get hardware info or set affinity, but if not coordinated, can lead to thrashing / over-taxed resources. Can work well for single applications. \cite{reinders2007intel}\cite{hellerstein2008optimizing}
\end{itemize}
\subsection{Even if you could infer information, active cooperation is impossible}
Maybe put this into the above section
\textbf{SEGUE}: show one problem of applications fending for themselves
\subsection{Talk about how this fits into a cloud environment}
For example: now we have yet another barrier: container/VM. So we are multiplexing over multiplexing over the underlying hardware.
