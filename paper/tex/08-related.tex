\section{Discussion and related work}
There are three broad areas of related work. First, there is a long and significant history of coordinating resources among applications. Second, there are several parallel programming libraries that touch on many similar ideas (e.g., monitoring resources, manipulating application behavior). Finally, from a broader perspective, there are the works that investigate and tease the application-OS interface.

\subsection{Coordinating resource consumption}
Coordinating resources is an important problem in many domains. In distributed systems alone, we have seen numerous takes on the problem \cite{yoo2003slurm}\cite{bhattacharya2013hierarchical}\cite{regehr2001using}\cite{chowdhury2014efficient}\cite{waterman2012coordinated}\cite{hindman2011mesos}\cite{ghodsi2011dominant}\cite{henderson1995job}. We acknowledge that we could not possibly mention all relevant approaches. Instead, we select a few diverse examples that specifically address a single node. We classify these approaches approximately as application-aware and hardware-aware.

Application-aware approaches have typically relied on application directives (e.g., \texttt{UNLIKELY}, processor affinity, DSLs like OpenCL), online monitoring, offline profiling, or some combination of the three. Gang scheduling~\cite{ousterhout1982scheduling} marks one of the earlier approaches to coordination, describing a mechanism for communicating units of execution to align and avoid blocking. Cache-~\cite{philbin1996thread} and contention-aware~\cite{blagodurov2010contention} schedulers examine historical application behavior to make intelligent resource allocations. Finally, the rich literature of real-time systems~\cite{liu1973scheduling}\cite{goyal1996hierarchical} demand very precise information from applications.

However, we observe that while these approaches make use of application information, they make decisions without the cooperation of applications. We believe that cooperative multitasking is most similar in spirit to our approach, but as evidenced by virtually all operating systems, it may have chosen dangerous abstractions.

% Unorganized
% \begin{itemize}
% \item \href{http://ethesis.inp-toulouse.fr/archive/00002747/01/tran.pdf}{resizing vms}: use VM's facilities to communicate new hardware reality to applications
% \item \href{https://www.usenix.org/legacy/events/osdi99/full_papers/banga/banga.pdf}{resource containers}
% \end{itemize}

By contrast, hardware-aware approaches attempt to probe information about underlying hardware~\cite{blagodurov2010case} \cite{topcuoglu2002performance} to make resource decisions. Perhaps most famously the Linux scheduler~\cite{molnar2007cfs} shifted from application-aware, with its complex system for classifying applications as interactive or batch, to hardware-aware (e.g.. via scheduling domains). While the Linux scheduler is remarkably effective at its unenviable task as a generic scheduler, it has its own challenges\cite{lozi2016linux}. We argue that the Linux scheduler need not take on additional complexity and that we can instead push responsibility to the applications themselves.

\subsection{Parallel programming libraries}
Many parallel programming libraries exist to ease the burden of concurrency on developers. While some might be considered more syntactic sugar, many are robust software projects in their own right, handling thread creation, destruction, scheduling, and more. However, to the authors' knowledge, we present the first POSIX scheduler that dynamically scales the level of application parallelism at run-time in response to fluctuating resources. Given our discussion of Intel's TBB, we highlight Microsoft's .NET Thread Pool as another advanced threading library.

The key insight from Microsoft's Common Language Runtime (CLR) .NET thread pool~\cite{hellerstein2008optimizing}\cite{hellerstein2009configuring} is to borrow ideas from control theory rather than observe system metrics that may not be related to application behavior. They monitor throughput of work queues and use closed-loop control and a hill-climbing algorithm to determine whether to add or remove threads. However, one of the primary motivations for this mechanism was the expense of maintaining unused threads in Windows.

% MxN threading (e.g., golang)
  
% \subsubsection{Intel Threading Building Blocks}
% Intel TBB \cite{contreras2008characterizing}: Reduce thread contention in a pretty simple way: check number of hardware threads in the beginning and default to only using that many threads. Primarily for CPU-intensive tasks. If you over-provision, make the extra threads sleep. However, this still requires over-provisioning, doesn't give you good insight if you're in a VM, doesn't coordinate with other apps, has its own overhead, etc. Documentation \href{https://software.intel.com/en-us/node/506294}{here}.
% \begin{itemize}
% \item Borrows many ideas (\href{https://software.intel.com/en-us/blogs/2007/08/13/threading-building-blocks-scheduling-and-task-stealing-introduction}{task stealing}, recursive tasks, unfair scheduling) from Cilk \cite{blumofe1995cilk} Danaher et al. 2005; Frigo et al. 1998, THE protocol (Frigo et al. 1998)
% \item Others from Charm++ \cite{kale1993charm++} / Chare \cite{kale1990chare} (advantage of breaking program into many small tasks; distributing load easier)
% \end{itemize}

% \subsubsection{Capriccio}
% Capriccio \cite{von2003capriccio} creates graph of blocking calls in an application with edge weights the time between those calls / annotated with resources used. Run queue for each node. Prioritize nodes for scheduling based on resource utilization and what nodes can relieve bottlenecks.

% \subsection{Application / OS interface}
% Because we are advocating for sharing more information between host OS and application. Note that the first three did not take off. BPF may have struck the balance better: get mainlined updates to Linux kernel, allow access to kernel in controlled way that exposes useful behaviors. We consider containered applications as motivation, but should also talk about VMs (since we make the argument that parallel libraries may not know underlying hardware / increasing abstraction).
% \begin{itemize}
%   \item Scheduler activations \cite{anderson1992scheduler}
%   \item Multikernel \cite{baumann2009multikernel}
%   \item Exokernel \cite{engler1995exokernel}
%   \item BPF
% \end{itemize}