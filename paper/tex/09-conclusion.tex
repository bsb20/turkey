\section{Conclusion}
\subsection{Summary}
We have argued the case for cooperative resource sharing, and presented an example implementation of it in the form of \mechfull{}s. In a world where parallelism and multi-tenancy are both increasing, the traditional abstraction that each application is the sole user of the system is becoming untenable. Attempts to solve this problem have hitherto tried to provide even stronger isolation guarantees with limited success under heavily parallel workloads.

We argue that weakening the isolation abstraction allows for applications to cooperate over the utilization of shared resources. By evaluating an implementation of cooperative resource sharing (\mech{}s) against the PARSEC parallel computing benchmark, we have shown that even naive cooperation over resources can yield performance benefits.

\subsection{Limitations}
This paper primarily seeks to make a case for a new direction in research, not to provide exhaustive proof that cooperative resource sharing is the optimal paradigm. To that end, our implementation is simple, and does not boast many basic features one would want in a production-ready system. 

Furthermore, the traditional downsides of cooperative multitasking models still apply. Malicious applications (or even just uncooperative ones) will likely ruin system performance. While these downsides are limited in many multi-tenant environments, there are many domains in which cooperation is not appropriate.

Lastly, we have only examined a specific kind of workload in a highly multi-tenant environment. The PARSEC benchmark aims to be a diverse representation of CPU-bound parallel workloads, but it is possible that our implementation over-fits to a synthetic benchmark and may not perform as well in a "real world" environment.

\subsection{Future work}
As cooperative resource sharing is relatively unexplored, substantial work needs to be done to validate that it is feasible under different workloads and with different resources under contention. More sophisticated scheduling policies can be tested to see whether cooperation can make the same kinds of tradeoffs about fairness, latency, and so on as a more traditional scheduling approach. Dynamic run-time parallelism should be evaluated against simpler approaches to application resource control, and concurrent programming paradigms that better support dynamic run-time parallelism should be demonstrated.