\section{Designing a CPU \mechfull{} for parallel programs}
This section discusses the design of a \mechfull{}, a system to enable cooperative resource sharing. This design is generalizable to any type of shared resource and any type of applciation, but in the interest of brevity we will discuss it in terms of sharing CPU resources between highly parallel applications.

\subsection{Message bus}
This component enables communication between the scheduler and client libraries hosted on each cooperating application.

It is important to note that, as a single component accessible to all applications, the message bus is itself a shared resource. Therefore, care must be taken that it does not become a bottleneck in the system.

To mitigate contention for this component, the scheduler and client libraries implement a stateful, asynchronous protocol over this message bus that allows them to operate independently of one another, decreasing the likelihood of synchronization overhead.
\subsection{Scheduler}
The scheduler is responsible for maintaining global resource state and allocating resource between cooperating applications in accordance with some predetermined policy.

In our example of a CPU-sharing \mech{}, the scheduler will decide how many software threads each application should be allowed.

\subsubsection{System poller}
To collect and store this information, the scheduler maintains a system poller that communicates with the operating system about hardware resources and stores the resulting data in a time-series data structure.

In the case of a CPU-sharing \mech, global resource state could, for example, consist of:
\begin{itemize}
    \item Number of cores and degree of hyperthreading
    \item Number of runnable processes
    \item Number of sleeping processes
\end{itemize}

\subsubsection{Application poller}
At the same time, the scheduler continuously checks for messages on the message bus. These messages provide confirmation that cooperating applications have indeed adjusted their behavior in accordance with the scheduler, as well as passing along any application-level data that the scheduling policy might require. 

Application-level data is likely to change over the life-time of application. This is especially true of applications with distinct phases of operation. Thus, it is sensible to store application-level data in a time-series data structure as well.

In the CPU-sharing case, the application would send the number of software threads it is operating with. Additional data shared could be, for example, application-specific performance instrumentation or user-specific SLAs.

\subsubsection{Scheduling policy}
Based on data gathered from the system poller and the application poller, the scheduler makes a decision on resource allocation to cooperating applications based on a its scheduling policy. These allocations are then communicated via the message bus to each application's client library.

The efficiency information of each application can be used by the policy achieve certain goals such as maximizing throughput, minimizing average latency, or maximizing hardware utilization. 

There is a wide body of operating systems research and industry work on the merits of various scheduling policies ~\cite{li2009scheduler}. We note that they could be implemented in any \mech{} scheduler.

\subsection{Client library}
The client library is hosted by all cooperating applications. In a typical cloud computing environment, all applications of non-trivial resource consumption will be cooperating, since the environment is under the full control of the system operator.
In the case of as CPU-sharing \mech{}, the client library has two modules:

\subsubsection{Message handler} 
This module periodically checks the message bus for any messages from the scheduler about how to adjust the application's resource consumption, and updates the client library's internal state accordingly. 

In the CPU-sharing case, the message from the scheduler takes the form of a recommendation for how many software threads to use.

\subsubsection{Resource controller}
This module takes the scheduler's recommendation about resource consumption and executes it by adjusting run-time application behavior. In the CPU-sharing case, this would take the form a thread pool which supports dynamic size adjustment. 

This is the module that is most tightly coupled with application logic. As a result, care must be taken to design the controller in a way that is easily integrated into existing systems. For example, the CPU-sharing resource controller could wrap a popular existing thread pool implementation, allowing clients to specify a starting size with the understanding that it may change over time. As another example, a memory-sharing resource controller could implement a dynamically-sized arena that applications use in place of ad-hoc memory allocation.
