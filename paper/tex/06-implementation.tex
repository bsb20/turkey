\section{Implementation considerations}
In this section, we discuss an example of a concrete implementation of a CPU-sharing \mechfull{} design. We begin by outlining some of our general design goals, and proceed by examining implementation considerations for each component.

\subsection{Design goals}
At a high level, we wanted our implementation to conform to the following goals:
\begin{itemize}
    \item \textbf{Low overhead.} As the primary purpose of cooperative resource sharing is to increase global application performance, care must be taken that our \mech{} implementation not become a practical bottleneck.
    \item \textbf{Graceful fallback.} To ease application integration, simplify testing, and increase robustness, our implementation should fall back to sensible default in the event that any component becomes unavailable.
\end{itemize}

In addition, we had the following non-goals:
\begin{itemize}
  \item \textbf{Exhaustive performance optimization.} Our goal was to make the case that cooperative resource sharing can improve performance for highly parallel workloads, not to tune a production environment for maximum efficiency.
  \item \textbf{Sophisticated scheduling policy.} Significant work has already been done in scheduling theory and practice. Our implementation need not retread existing ground to prove that cooperation is desirable.
  \item \textbf{Intrusive resource control.} While it may be tempting to specify elaborate resource acquisition and consumption requirements for applications, in practice this would require that developers substantially revise their computation models in order to integrate with our library.
\end{itemize}

\subsection{Message bus}
As a globally shared resource, message bus must be fast even in the presence of many clients. In practice, this means any implementation must achieve:
\begin{itemize}
    \item \textbf{Fast reads and writes.} An interprocess communication scheme must be chosen such that both the scheduler and client libraries have extremely fast access.
    \item \textbf{Low synchronization overhead.} Even with many (hundreds) of applications, there should be little or no contention for synchronization resources..
\end{itemize}

To achieve fast and reads and writes, we implement the message bus using shared memory as our communication medium. We create two channels in the allocated shared memory, corresponding to each direction of communication.

The scheduler-to-application channel contains messages regarding the scheduler's latest recommendation for the number of software threads for applications to use. The application-to-scheduler channel contains messages where applications register their existence with the scheduler and update their current thread count.

Both channels are protected by reader-writer locks to ensure that multiple readers do not contend for access to the channel. In our implementation, the client library only sends and receives messages every 500 milliseconds, which makes lock contention negligible in practice. If the latency of client response is a concern, finer-grained synchronization models are feasible (for example, giving each application it's own channel).

\subsection{Scheduler}
\subsubsection{Scheduling policy}
As sophisticated scheduling was a non-goal, we intentionally made our scheduler policy as minimal as possible. Our policy only requires one piece of system information to make allocation decisions: the number of runnable processes currently waiting to run. On this basis, a single recommendation is issued for all applications to follow. No distinctions are made between applications, and the scheduling policy only tries to maximize global application throughput.

\subsubsection{System poller}
For low overhead communication between the kernel and userspace, we rely on \texttt{procfs}. This allows the system poller to gather basic information about the system without the overhead of a system call or some dynamic tracing method. Our implementation only reads the \texttt{procs\_running} field of \texttt{/proc/stat}, which gives the number of threads currently running on CPU.

This information is gathered once per second and saved in a fixed-size time series data structure.

\subsubsection{Application poller} 
Our simple scheduling policy does not require information from the application. While we implemented a channel for the application to pass messages to the scheduler about the current number of threads it is using, this information is unused by the scheduler.

\subsection{Client library}
In implementing the client library, a decision needs to be made about when the resource controller will intervene to set the number of threads in the application. There are three options, each with varying levels of intrusion into the application logic:

\begin{enumerate}
  \item \textbf{Static parallelism.} This is equivalent to no resource controller at all; the number of threads is determined by user input.
  \item \textbf{Dynamic parallelism.} On start-up, the client library consults the scheduler for how many threads to use. This number is kept throughout the lifetime to the application.
  \item \textbf{Dynamic run-time parallelism.} Throughout the lifetime of the application, the client library continuously receives messages from the scheduler and resizes its thread pool accordingly.
\end{enumerate}

While dynamic run-time parallelism is the optimal resource control model from a performance perspective, it is highly intrusive and often requires the application developer to re-architect the concurrency model of their program. 

For example, the \texttt{canneal} benchmark in PARSEC uses a highly aggressive lock-free synchronization strategy that recovers from data races rather than try to avoid them~\cite{bienia2008parsec}. Changing the number of threads dynamically would require substantial effort, and repartitioning the work when changing thread count would require global locking that would sacrifice the performance benefits of a lock-free strategy.

Thus, while we implemented the features necessary for dynamic run-time parallelism in our client library, we opted to include a mode that disabled the run-time thread count updates. 
